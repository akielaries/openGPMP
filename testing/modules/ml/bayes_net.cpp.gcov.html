<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>LCOV - lcov.info - modules/ml/bayes_net.cpp</title>
  <link rel="stylesheet" type="text/css" href="../../gcov.css">
</head>

<body>

  <table width="100%" border=0 cellspacing=0 cellpadding=0>
    <tr><td class="title">LCOV - code coverage report</td></tr>
    <tr><td class="ruler"><img src="../../glass.png" width=3 height=3 alt=""></td></tr>

    <tr>
      <td width="100%">
        <table cellpadding=1 border=0 width="100%">
          <tr>
            <td width="10%" class="headerItem">Current view:</td>
            <td width="35%" class="headerValue"><a href="../../index.html">top level</a> - <a href="index.html">modules/ml</a> - bayes_net.cpp<span style="font-size: 80%;"> (source / <a href="bayes_net.cpp.func-sort-c.html">functions</a>)</span></td>
            <td width="5%"></td>
            <td width="15%"></td>
            <td width="10%" class="headerCovTableHead">Hit</td>
            <td width="10%" class="headerCovTableHead">Total</td>
            <td width="15%" class="headerCovTableHead">Coverage</td>
          </tr>
          <tr>
            <td class="headerItem">Test:</td>
            <td class="headerValue">lcov.info</td>
            <td></td>
            <td class="headerItem">Lines:</td>
            <td class="headerCovTableEntry">0</td>
            <td class="headerCovTableEntry">127</td>
            <td class="headerCovTableEntryLo">0.0 %</td>
          </tr>
          <tr>
            <td class="headerItem">Date:</td>
            <td class="headerValue">2024-02-07 03:30:17</td>
            <td></td>
            <td class="headerItem">Functions:</td>
            <td class="headerCovTableEntry">0</td>
            <td class="headerCovTableEntry">8</td>
            <td class="headerCovTableEntryLo">0.0 %</td>
          </tr>
          <tr>
            <td class="headerItem">Legend:</td>
            <td class="headerValueLeg">            Lines:
            <span class="coverLegendCov">hit</span>
            <span class="coverLegendNoCov">not hit</span>
</td>
            <td></td>
          </tr>
          <tr><td><img src="../../glass.png" width=3 height=3 alt=""></td></tr>
        </table>
      </td>
    </tr>

    <tr><td class="ruler"><img src="../../glass.png" width=3 height=3 alt=""></td></tr>
  </table>

  <table cellpadding=0 cellspacing=0 border=0>
    <tr>
      <td><br></td>
    </tr>
    <tr>
      <td>
<pre class="sourceHeading">          Line data    Source code</pre>
<pre class="source">
<a name="1"><span class="lineNum">       1 </span>            : /*************************************************************************</a>
<a name="2"><span class="lineNum">       2 </span>            :  *</a>
<a name="3"><span class="lineNum">       3 </span>            :  *  Project</a>
<a name="4"><span class="lineNum">       4 </span>            :  *                         _____ _____  __  __ _____</a>
<a name="5"><span class="lineNum">       5 </span>            :  *                        / ____|  __ \|  \/  |  __ \</a>
<a name="6"><span class="lineNum">       6 </span>            :  *  ___  _ __   ___ _ __ | |  __| |__) | \  / | |__) |</a>
<a name="7"><span class="lineNum">       7 </span>            :  * / _ \| '_ \ / _ \ '_ \| | |_ |  ___/| |\/| |  ___/</a>
<a name="8"><span class="lineNum">       8 </span>            :  *| (_) | |_) |  __/ | | | |__| | |    | |  | | |</a>
<a name="9"><span class="lineNum">       9 </span>            :  * \___/| .__/ \___|_| |_|\_____|_|    |_|  |_|_|</a>
<a name="10"><span class="lineNum">      10 </span>            :  *      | |</a>
<a name="11"><span class="lineNum">      11 </span>            :  *      |_|</a>
<a name="12"><span class="lineNum">      12 </span>            :  *</a>
<a name="13"><span class="lineNum">      13 </span>            :  * Copyright (C) Akiel Aries, &lt;akiel@akiel.org&gt;, et al.</a>
<a name="14"><span class="lineNum">      14 </span>            :  *</a>
<a name="15"><span class="lineNum">      15 </span>            :  * This software is licensed as described in the file LICENSE, which</a>
<a name="16"><span class="lineNum">      16 </span>            :  * you should have received as part of this distribution. The terms</a>
<a name="17"><span class="lineNum">      17 </span>            :  * among other details are referenced in the official documentation</a>
<a name="18"><span class="lineNum">      18 </span>            :  * seen here : https://akielaries.github.io/openGPMP/ along with</a>
<a name="19"><span class="lineNum">      19 </span>            :  * important files seen in this project.</a>
<a name="20"><span class="lineNum">      20 </span>            :  *</a>
<a name="21"><span class="lineNum">      21 </span>            :  * You may opt to use, copy, modify, merge, publish, distribute</a>
<a name="22"><span class="lineNum">      22 </span>            :  * and/or sell copies of the Software, and permit persons to whom</a>
<a name="23"><span class="lineNum">      23 </span>            :  * the Software is furnished to do so, under the terms of the</a>
<a name="24"><span class="lineNum">      24 </span>            :  * LICENSE file. As this is an Open Source effort, all implementations</a>
<a name="25"><span class="lineNum">      25 </span>            :  * must be of the same methodology.</a>
<a name="26"><span class="lineNum">      26 </span>            :  *</a>
<a name="27"><span class="lineNum">      27 </span>            :  *</a>
<a name="28"><span class="lineNum">      28 </span>            :  *</a>
<a name="29"><span class="lineNum">      29 </span>            :  * This software is distributed on an AS IS basis, WITHOUT</a>
<a name="30"><span class="lineNum">      30 </span>            :  * WARRANTY OF ANY KIND, either express or implied.</a>
<a name="31"><span class="lineNum">      31 </span>            :  *</a>
<a name="32"><span class="lineNum">      32 </span>            :  ************************************************************************/</a>
<a name="33"><span class="lineNum">      33 </span>            : </a>
<a name="34"><span class="lineNum">      34 </span>            : #include &quot;../../include/ml/bayes_net.hpp&quot;</a>
<a name="35"><span class="lineNum">      35 </span>            : #include &lt;iostream&gt;</a>
<a name="36"><span class="lineNum">      36 </span>            : </a>
<a name="37"><span class="lineNum">      37 </span><span class="lineNoCov">          0 : BNN::BNN(int in_size, int h_size, int out_size, double p_variance)</span></a>
<a name="38"><span class="lineNum">      38 </span><span class="lineNoCov">          0 :     : input_size(in_size), hidden_size(h_size), output_size(out_size),</span></a>
<a name="39"><span class="lineNum">      39 </span><span class="lineNoCov">          0 :       prior_variance(p_variance), rng(std::random_device{}()) {</span></a>
<a name="40"><span class="lineNum">      40 </span>            :     // initialize weights and biases with random values</a>
<a name="41"><span class="lineNum">      41 </span><span class="lineNoCov">          0 :     std::normal_distribution&lt;double&gt; normal_distribution(0.0, 1.0);</span></a>
<a name="42"><span class="lineNum">      42 </span>            : </a>
<a name="43"><span class="lineNum">      43 </span><span class="lineNoCov">          0 :     input_to_hidden_weights.resize(hidden_size,</span></a>
<a name="44"><span class="lineNum">      44 </span><span class="lineNoCov">          0 :                                    std::vector&lt;double&gt;(input_size));</span></a>
<a name="45"><span class="lineNum">      45 </span><span class="lineNoCov">          0 :     for (int i = 0; i &lt; hidden_size; ++i) {</span></a>
<a name="46"><span class="lineNum">      46 </span><span class="lineNoCov">          0 :         for (int j = 0; j &lt; input_size; ++j) {</span></a>
<a name="47"><span class="lineNum">      47 </span><span class="lineNoCov">          0 :             input_to_hidden_weights[i][j] = normal_distribution(rng);</span></a>
<a name="48"><span class="lineNum">      48 </span>            :         }</a>
<a name="49"><span class="lineNum">      49 </span>            :     }</a>
<a name="50"><span class="lineNum">      50 </span>            : </a>
<a name="51"><span class="lineNum">      51 </span><span class="lineNoCov">          0 :     hidden_biases.resize(hidden_size);</span></a>
<a name="52"><span class="lineNum">      52 </span><span class="lineNoCov">          0 :     for (int i = 0; i &lt; hidden_size; ++i) {</span></a>
<a name="53"><span class="lineNum">      53 </span><span class="lineNoCov">          0 :         hidden_biases[i] = normal_distribution(rng);</span></a>
<a name="54"><span class="lineNum">      54 </span>            :     }</a>
<a name="55"><span class="lineNum">      55 </span>            : </a>
<a name="56"><span class="lineNum">      56 </span><span class="lineNoCov">          0 :     hidden_to_output_weights.resize(output_size,</span></a>
<a name="57"><span class="lineNum">      57 </span><span class="lineNoCov">          0 :                                     std::vector&lt;double&gt;(hidden_size));</span></a>
<a name="58"><span class="lineNum">      58 </span><span class="lineNoCov">          0 :     for (int i = 0; i &lt; output_size; ++i) {</span></a>
<a name="59"><span class="lineNum">      59 </span><span class="lineNoCov">          0 :         for (int j = 0; j &lt; hidden_size; ++j) {</span></a>
<a name="60"><span class="lineNum">      60 </span><span class="lineNoCov">          0 :             hidden_to_output_weights[i][j] = normal_distribution(rng);</span></a>
<a name="61"><span class="lineNum">      61 </span>            :         }</a>
<a name="62"><span class="lineNum">      62 </span>            :     }</a>
<a name="63"><span class="lineNum">      63 </span>            : </a>
<a name="64"><span class="lineNum">      64 </span><span class="lineNoCov">          0 :     output_biases.resize(output_size);</span></a>
<a name="65"><span class="lineNum">      65 </span><span class="lineNoCov">          0 :     for (int i = 0; i &lt; output_size; ++i) {</span></a>
<a name="66"><span class="lineNum">      66 </span><span class="lineNoCov">          0 :         output_biases[i] = normal_distribution(rng);</span></a>
<a name="67"><span class="lineNum">      67 </span>            :     }</a>
<a name="68"><span class="lineNum">      68 </span><span class="lineNoCov">          0 : }</span></a>
<a name="69"><span class="lineNum">      69 </span>            : </a>
<a name="70"><span class="lineNum">      70 </span><span class="lineNoCov">          0 : double BNN::activation_function(double x) {</span></a>
<a name="71"><span class="lineNum">      71 </span>            :     // Using sigmoid activation function for simplicity</a>
<a name="72"><span class="lineNum">      72 </span><span class="lineNoCov">          0 :     return 1.0 / (1.0 + std::exp(-x));</span></a>
<a name="73"><span class="lineNum">      73 </span>            : }</a>
<a name="74"><span class="lineNum">      74 </span>            : </a>
<a name="75"><span class="lineNum">      75 </span><span class="lineNoCov">          0 : void BNN::fit(const std::vector&lt;std::vector&lt;double&gt;&gt; &amp;X_train,</span></a>
<a name="76"><span class="lineNum">      76 </span>            :               const std::vector&lt;std::vector&lt;double&gt;&gt; &amp;y_train,</a>
<a name="77"><span class="lineNum">      77 </span>            :               int epochs) {</a>
<a name="78"><span class="lineNum">      78 </span><span class="lineNoCov">          0 :     const double learning_rate = 0.01;</span></a>
<a name="79"><span class="lineNum">      79 </span>            : </a>
<a name="80"><span class="lineNum">      80 </span><span class="lineNoCov">          0 :     for (int epoch = 0; epoch &lt; epochs; ++epoch) {</span></a>
<a name="81"><span class="lineNum">      81 </span><span class="lineNoCov">          0 :         update_weights(X_train, y_train, learning_rate);</span></a>
<a name="82"><span class="lineNum">      82 </span>            : </a>
<a name="83"><span class="lineNum">      83 </span>            :         // Print loss for monitoring training progress</a>
<a name="84"><span class="lineNum">      84 </span><span class="lineNoCov">          0 :         if (epoch % 100 == 0) {</span></a>
<a name="85"><span class="lineNum">      85 </span><span class="lineNoCov">          0 :             double loss = compute_loss(X_train, y_train);</span></a>
<a name="86"><span class="lineNum">      86 </span><span class="lineNoCov">          0 :             std::cout &lt;&lt; &quot;Epoch: &quot; &lt;&lt; epoch &lt;&lt; &quot;, Loss: &quot; &lt;&lt; loss &lt;&lt; std::endl;</span></a>
<a name="87"><span class="lineNum">      87 </span>            :         }</a>
<a name="88"><span class="lineNum">      88 </span>            :     }</a>
<a name="89"><span class="lineNum">      89 </span><span class="lineNoCov">          0 : }</span></a>
<a name="90"><span class="lineNum">      90 </span>            : </a>
<a name="91"><span class="lineNum">      91 </span><span class="lineNoCov">          0 : std::vector&lt;double&gt; BNN::predict(const std::vector&lt;double&gt; &amp;input_vector) {</span></a>
<a name="92"><span class="lineNum">      92 </span>            :     // Forward pass</a>
<a name="93"><span class="lineNum">      93 </span><span class="lineNoCov">          0 :     std::vector&lt;double&gt; hidden_output(hidden_size);</span></a>
<a name="94"><span class="lineNum">      94 </span><span class="lineNoCov">          0 :     for (int i = 0; i &lt; hidden_size; ++i) {</span></a>
<a name="95"><span class="lineNum">      95 </span><span class="lineNoCov">          0 :         hidden_output[i] = activation_function(</span></a>
<a name="96"><span class="lineNum">      96 </span><span class="lineNoCov">          0 :             std::inner_product(input_vector.begin(),</span></a>
<a name="97"><span class="lineNum">      97 </span>            :                                input_vector.end(),</a>
<a name="98"><span class="lineNum">      98 </span><span class="lineNoCov">          0 :                                input_to_hidden_weights[i].begin(),</span></a>
<a name="99"><span class="lineNum">      99 </span>            :                                0.0) +</a>
<a name="100"><span class="lineNum">     100 </span><span class="lineNoCov">          0 :             hidden_biases[i]);</span></a>
<a name="101"><span class="lineNum">     101 </span>            :     }</a>
<a name="102"><span class="lineNum">     102 </span>            : </a>
<a name="103"><span class="lineNum">     103 </span><span class="lineNoCov">          0 :     std::vector&lt;double&gt; output(output_size);</span></a>
<a name="104"><span class="lineNum">     104 </span><span class="lineNoCov">          0 :     for (int i = 0; i &lt; output_size; ++i) {</span></a>
<a name="105"><span class="lineNum">     105 </span><span class="lineNoCov">          0 :         output[i] = activation_function(</span></a>
<a name="106"><span class="lineNum">     106 </span><span class="lineNoCov">          0 :             std::inner_product(hidden_output.begin(),</span></a>
<a name="107"><span class="lineNum">     107 </span>            :                                hidden_output.end(),</a>
<a name="108"><span class="lineNum">     108 </span><span class="lineNoCov">          0 :                                hidden_to_output_weights[i].begin(),</span></a>
<a name="109"><span class="lineNum">     109 </span>            :                                0.0) +</a>
<a name="110"><span class="lineNum">     110 </span><span class="lineNoCov">          0 :             output_biases[i]);</span></a>
<a name="111"><span class="lineNum">     111 </span>            :     }</a>
<a name="112"><span class="lineNum">     112 </span>            : </a>
<a name="113"><span class="lineNum">     113 </span><span class="lineNoCov">          0 :     return output;</span></a>
<a name="114"><span class="lineNum">     114 </span><span class="lineNoCov">          0 : }</span></a>
<a name="115"><span class="lineNum">     115 </span>            : </a>
<a name="116"><span class="lineNum">     116 </span><span class="lineNoCov">          0 : double BNN::prior_log_likelihood() {</span></a>
<a name="117"><span class="lineNum">     117 </span>            :     // Compute log-likelihood of the prior</a>
<a name="118"><span class="lineNum">     118 </span><span class="lineNoCov">          0 :     double log_likelihood = 0.0;</span></a>
<a name="119"><span class="lineNum">     119 </span>            : </a>
<a name="120"><span class="lineNum">     120 </span>            :     // Prior for input-to-hidden weights</a>
<a name="121"><span class="lineNum">     121 </span><span class="lineNoCov">          0 :     for (int i = 0; i &lt; hidden_size; ++i) {</span></a>
<a name="122"><span class="lineNum">     122 </span><span class="lineNoCov">          0 :         for (int j = 0; j &lt; input_size; ++j) {</span></a>
<a name="123"><span class="lineNum">     123 </span><span class="lineNoCov">          0 :             log_likelihood +=</span></a>
<a name="124"><span class="lineNum">     124 </span><span class="lineNoCov">          0 :                 -0.5 *</span></a>
<a name="125"><span class="lineNum">     125 </span><span class="lineNoCov">          0 :                 (std::pow(input_to_hidden_weights[i][j], 2) / prior_variance +</span></a>
<a name="126"><span class="lineNum">     126 </span><span class="lineNoCov">          0 :                  std::log(2 * M_PI * prior_variance));</span></a>
<a name="127"><span class="lineNum">     127 </span>            :         }</a>
<a name="128"><span class="lineNum">     128 </span>            :     }</a>
<a name="129"><span class="lineNum">     129 </span>            : </a>
<a name="130"><span class="lineNum">     130 </span>            :     // Prior for hidden biases</a>
<a name="131"><span class="lineNum">     131 </span><span class="lineNoCov">          0 :     for (int i = 0; i &lt; hidden_size; ++i) {</span></a>
<a name="132"><span class="lineNum">     132 </span><span class="lineNoCov">          0 :         log_likelihood +=</span></a>
<a name="133"><span class="lineNum">     133 </span><span class="lineNoCov">          0 :             -0.5 * (std::pow(hidden_biases[i], 2) / prior_variance +</span></a>
<a name="134"><span class="lineNum">     134 </span><span class="lineNoCov">          0 :                     std::log(2 * M_PI * prior_variance));</span></a>
<a name="135"><span class="lineNum">     135 </span>            :     }</a>
<a name="136"><span class="lineNum">     136 </span>            : </a>
<a name="137"><span class="lineNum">     137 </span>            :     // Prior for hidden-to-output weights</a>
<a name="138"><span class="lineNum">     138 </span><span class="lineNoCov">          0 :     for (int i = 0; i &lt; output_size; ++i) {</span></a>
<a name="139"><span class="lineNum">     139 </span><span class="lineNoCov">          0 :         for (int j = 0; j &lt; hidden_size; ++j) {</span></a>
<a name="140"><span class="lineNum">     140 </span><span class="lineNoCov">          0 :             log_likelihood +=</span></a>
<a name="141"><span class="lineNum">     141 </span><span class="lineNoCov">          0 :                 -0.5 *</span></a>
<a name="142"><span class="lineNum">     142 </span><span class="lineNoCov">          0 :                 (std::pow(hidden_to_output_weights[i][j], 2) / prior_variance +</span></a>
<a name="143"><span class="lineNum">     143 </span><span class="lineNoCov">          0 :                  std::log(2 * M_PI * prior_variance));</span></a>
<a name="144"><span class="lineNum">     144 </span>            :         }</a>
<a name="145"><span class="lineNum">     145 </span>            :     }</a>
<a name="146"><span class="lineNum">     146 </span>            : </a>
<a name="147"><span class="lineNum">     147 </span>            :     // Prior for output biases</a>
<a name="148"><span class="lineNum">     148 </span><span class="lineNoCov">          0 :     for (int i = 0; i &lt; output_size; ++i) {</span></a>
<a name="149"><span class="lineNum">     149 </span><span class="lineNoCov">          0 :         log_likelihood +=</span></a>
<a name="150"><span class="lineNum">     150 </span><span class="lineNoCov">          0 :             -0.5 * (std::pow(output_biases[i], 2) / prior_variance +</span></a>
<a name="151"><span class="lineNum">     151 </span><span class="lineNoCov">          0 :                     std::log(2 * M_PI * prior_variance));</span></a>
<a name="152"><span class="lineNum">     152 </span>            :     }</a>
<a name="153"><span class="lineNum">     153 </span>            : </a>
<a name="154"><span class="lineNum">     154 </span><span class="lineNoCov">          0 :     return log_likelihood;</span></a>
<a name="155"><span class="lineNum">     155 </span>            : }</a>
<a name="156"><span class="lineNum">     156 </span>            : </a>
<a name="157"><span class="lineNum">     157 </span><span class="lineNoCov">          0 : double BNN::log_likelihood(const std::vector&lt;std::vector&lt;double&gt;&gt; &amp;X,</span></a>
<a name="158"><span class="lineNum">     158 </span>            :                            const std::vector&lt;std::vector&lt;double&gt;&gt; &amp;y) {</a>
<a name="159"><span class="lineNum">     159 </span>            :     // Compute log-likelihood of the data</a>
<a name="160"><span class="lineNum">     160 </span><span class="lineNoCov">          0 :     double log_likelihood = 0.0;</span></a>
<a name="161"><span class="lineNum">     161 </span>            : </a>
<a name="162"><span class="lineNum">     162 </span><span class="lineNoCov">          0 :     for (size_t i = 0; i &lt; X.size(); ++i) {</span></a>
<a name="163"><span class="lineNum">     163 </span>            :         // Forward pass</a>
<a name="164"><span class="lineNum">     164 </span><span class="lineNoCov">          0 :         std::vector&lt;double&gt; hidden_output(hidden_size);</span></a>
<a name="165"><span class="lineNum">     165 </span><span class="lineNoCov">          0 :         for (int j = 0; j &lt; hidden_size; ++j) {</span></a>
<a name="166"><span class="lineNum">     166 </span><span class="lineNoCov">          0 :             hidden_output[j] = activation_function(</span></a>
<a name="167"><span class="lineNum">     167 </span><span class="lineNoCov">          0 :                 std::inner_product(X[i].begin(),</span></a>
<a name="168"><span class="lineNum">     168 </span><span class="lineNoCov">          0 :                                    X[i].end(),</span></a>
<a name="169"><span class="lineNum">     169 </span><span class="lineNoCov">          0 :                                    input_to_hidden_weights[j].begin(),</span></a>
<a name="170"><span class="lineNum">     170 </span>            :                                    0.0) +</a>
<a name="171"><span class="lineNum">     171 </span><span class="lineNoCov">          0 :                 hidden_biases[j]);</span></a>
<a name="172"><span class="lineNum">     172 </span>            :         }</a>
<a name="173"><span class="lineNum">     173 </span>            : </a>
<a name="174"><span class="lineNum">     174 </span><span class="lineNoCov">          0 :         std::vector&lt;double&gt; output(output_size);</span></a>
<a name="175"><span class="lineNum">     175 </span><span class="lineNoCov">          0 :         for (int j = 0; j &lt; output_size; ++j) {</span></a>
<a name="176"><span class="lineNum">     176 </span><span class="lineNoCov">          0 :             output[j] = activation_function(</span></a>
<a name="177"><span class="lineNum">     177 </span><span class="lineNoCov">          0 :                 std::inner_product(hidden_output.begin(),</span></a>
<a name="178"><span class="lineNum">     178 </span>            :                                    hidden_output.end(),</a>
<a name="179"><span class="lineNum">     179 </span><span class="lineNoCov">          0 :                                    hidden_to_output_weights[j].begin(),</span></a>
<a name="180"><span class="lineNum">     180 </span>            :                                    0.0) +</a>
<a name="181"><span class="lineNum">     181 </span><span class="lineNoCov">          0 :                 output_biases[j]);</span></a>
<a name="182"><span class="lineNum">     182 </span>            :         }</a>
<a name="183"><span class="lineNum">     183 </span>            : </a>
<a name="184"><span class="lineNum">     184 </span>            :         // Log-likelihood for Gaussian noise</a>
<a name="185"><span class="lineNum">     185 </span><span class="lineNoCov">          0 :         for (int j = 0; j &lt; output_size; ++j) {</span></a>
<a name="186"><span class="lineNum">     186 </span><span class="lineNoCov">          0 :             log_likelihood +=</span></a>
<a name="187"><span class="lineNum">     187 </span><span class="lineNoCov">          0 :                 -0.5 * std::log(2 * M_PI) - 0.5 * std::log(prior_variance) -</span></a>
<a name="188"><span class="lineNum">     188 </span><span class="lineNoCov">          0 :                 0.5 * std::pow(y[i][j] - output[j], 2) / prior_variance;</span></a>
<a name="189"><span class="lineNum">     189 </span>            :         }</a>
<a name="190"><span class="lineNum">     190 </span><span class="lineNoCov">          0 :     }</span></a>
<a name="191"><span class="lineNum">     191 </span>            : </a>
<a name="192"><span class="lineNum">     192 </span><span class="lineNoCov">          0 :     return log_likelihood;</span></a>
<a name="193"><span class="lineNum">     193 </span>            : }</a>
<a name="194"><span class="lineNum">     194 </span>            : </a>
<a name="195"><span class="lineNum">     195 </span><span class="lineNoCov">          0 : double BNN::compute_loss(const std::vector&lt;std::vector&lt;double&gt;&gt; &amp;X,</span></a>
<a name="196"><span class="lineNum">     196 </span>            :                          const std::vector&lt;std::vector&lt;double&gt;&gt; &amp;y) {</a>
<a name="197"><span class="lineNum">     197 </span>            :     // Negative log posterior (loss function)</a>
<a name="198"><span class="lineNum">     198 </span><span class="lineNoCov">          0 :     return -log_likelihood(X, y) - prior_log_likelihood();</span></a>
<a name="199"><span class="lineNum">     199 </span>            : }</a>
<a name="200"><span class="lineNum">     200 </span>            : </a>
<a name="201"><span class="lineNum">     201 </span><span class="lineNoCov">          0 : void BNN::update_weights(const std::vector&lt;std::vector&lt;double&gt;&gt; &amp;X,</span></a>
<a name="202"><span class="lineNum">     202 </span>            :                          const std::vector&lt;std::vector&lt;double&gt;&gt; &amp;y,</a>
<a name="203"><span class="lineNum">     203 </span>            :                          double learning_rate) {</a>
<a name="204"><span class="lineNum">     204 </span>            :     // Update weights using stochastic gradient descent</a>
<a name="205"><span class="lineNum">     205 </span><span class="lineNoCov">          0 :     for (size_t i = 0; i &lt; X.size(); ++i) {</span></a>
<a name="206"><span class="lineNum">     206 </span>            :         // Forward pass</a>
<a name="207"><span class="lineNum">     207 </span><span class="lineNoCov">          0 :         std::vector&lt;double&gt; hidden_output(hidden_size);</span></a>
<a name="208"><span class="lineNum">     208 </span><span class="lineNoCov">          0 :         for (int j = 0; j &lt; hidden_size; ++j) {</span></a>
<a name="209"><span class="lineNum">     209 </span><span class="lineNoCov">          0 :             hidden_output[j] = activation_function(</span></a>
<a name="210"><span class="lineNum">     210 </span><span class="lineNoCov">          0 :                 std::inner_product(X[i].begin(),</span></a>
<a name="211"><span class="lineNum">     211 </span><span class="lineNoCov">          0 :                                    X[i].end(),</span></a>
<a name="212"><span class="lineNum">     212 </span><span class="lineNoCov">          0 :                                    input_to_hidden_weights[j].begin(),</span></a>
<a name="213"><span class="lineNum">     213 </span>            :                                    0.0) +</a>
<a name="214"><span class="lineNum">     214 </span><span class="lineNoCov">          0 :                 hidden_biases[j]);</span></a>
<a name="215"><span class="lineNum">     215 </span>            :         }</a>
<a name="216"><span class="lineNum">     216 </span>            : </a>
<a name="217"><span class="lineNum">     217 </span><span class="lineNoCov">          0 :         std::vector&lt;double&gt; output(output_size);</span></a>
<a name="218"><span class="lineNum">     218 </span><span class="lineNoCov">          0 :         for (int j = 0; j &lt; output_size; ++j) {</span></a>
<a name="219"><span class="lineNum">     219 </span><span class="lineNoCov">          0 :             output[j] = activation_function(</span></a>
<a name="220"><span class="lineNum">     220 </span><span class="lineNoCov">          0 :                 std::inner_product(hidden_output.begin(),</span></a>
<a name="221"><span class="lineNum">     221 </span>            :                                    hidden_output.end(),</a>
<a name="222"><span class="lineNum">     222 </span><span class="lineNoCov">          0 :                                    hidden_to_output_weights[j].begin(),</span></a>
<a name="223"><span class="lineNum">     223 </span>            :                                    0.0) +</a>
<a name="224"><span class="lineNum">     224 </span><span class="lineNoCov">          0 :                 output_biases[j]);</span></a>
<a name="225"><span class="lineNum">     225 </span>            :         }</a>
<a name="226"><span class="lineNum">     226 </span>            : </a>
<a name="227"><span class="lineNum">     227 </span>            :         // Backward pass (stochastic gradient descent)</a>
<a name="228"><span class="lineNum">     228 </span><span class="lineNoCov">          0 :         for (int j = 0; j &lt; output_size; ++j) {</span></a>
<a name="229"><span class="lineNum">     229 </span>            :             // Gradient for output layer</a>
<a name="230"><span class="lineNum">     230 </span>            :             double output_gradient =</a>
<a name="231"><span class="lineNum">     231 </span><span class="lineNoCov">          0 :                 (y[i][j] - output[j]) * output[j] * (1.0 - output[j]);</span></a>
<a name="232"><span class="lineNum">     232 </span>            : </a>
<a name="233"><span class="lineNum">     233 </span>            :             // Update hidden-to-output weights</a>
<a name="234"><span class="lineNum">     234 </span><span class="lineNoCov">          0 :             for (int k = 0; k &lt; hidden_size; ++k) {</span></a>
<a name="235"><span class="lineNum">     235 </span><span class="lineNoCov">          0 :                 hidden_to_output_weights[j][k] +=</span></a>
<a name="236"><span class="lineNum">     236 </span><span class="lineNoCov">          0 :                     learning_rate * output_gradient * hidden_output[k];</span></a>
<a name="237"><span class="lineNum">     237 </span>            :             }</a>
<a name="238"><span class="lineNum">     238 </span>            : </a>
<a name="239"><span class="lineNum">     239 </span>            :             // Update output biases</a>
<a name="240"><span class="lineNum">     240 </span><span class="lineNoCov">          0 :             output_biases[j] += learning_rate * output_gradient;</span></a>
<a name="241"><span class="lineNum">     241 </span>            :         }</a>
<a name="242"><span class="lineNum">     242 </span>            : </a>
<a name="243"><span class="lineNum">     243 </span>            :         // Backward pass for hidden layer</a>
<a name="244"><span class="lineNum">     244 </span><span class="lineNoCov">          0 :         for (int j = 0; j &lt; hidden_size; ++j) {</span></a>
<a name="245"><span class="lineNum">     245 </span>            :             // Gradient for hidden layer</a>
<a name="246"><span class="lineNum">     246 </span><span class="lineNoCov">          0 :             double hidden_gradient = 0.0;</span></a>
<a name="247"><span class="lineNum">     247 </span><span class="lineNoCov">          0 :             for (int k = 0; k &lt; output_size; ++k) {</span></a>
<a name="248"><span class="lineNum">     248 </span><span class="lineNoCov">          0 :                 hidden_gradient += input_to_hidden_weights[k][j] *</span></a>
<a name="249"><span class="lineNum">     249 </span><span class="lineNoCov">          0 :                                    (y[i][k] - output[k]) * output[k] *</span></a>
<a name="250"><span class="lineNum">     250 </span><span class="lineNoCov">          0 :                                    (1.0 - output[k]);</span></a>
<a name="251"><span class="lineNum">     251 </span>            :             }</a>
<a name="252"><span class="lineNum">     252 </span><span class="lineNoCov">          0 :             hidden_gradient *= hidden_output[j] * (1.0 - hidden_output[j]);</span></a>
<a name="253"><span class="lineNum">     253 </span>            : </a>
<a name="254"><span class="lineNum">     254 </span>            :             // Update input-to-hidden weights</a>
<a name="255"><span class="lineNum">     255 </span><span class="lineNoCov">          0 :             for (int k = 0; k &lt; input_size; ++k) {</span></a>
<a name="256"><span class="lineNum">     256 </span><span class="lineNoCov">          0 :                 input_to_hidden_weights[j][k] +=</span></a>
<a name="257"><span class="lineNum">     257 </span><span class="lineNoCov">          0 :                     learning_rate * hidden_gradient * X[i][k];</span></a>
<a name="258"><span class="lineNum">     258 </span>            :             }</a>
<a name="259"><span class="lineNum">     259 </span>            : </a>
<a name="260"><span class="lineNum">     260 </span>            :             // Update hidden biases</a>
<a name="261"><span class="lineNum">     261 </span><span class="lineNoCov">          0 :             hidden_biases[j] += learning_rate * hidden_gradient;</span></a>
<a name="262"><span class="lineNum">     262 </span>            :         }</a>
<a name="263"><span class="lineNum">     263 </span><span class="lineNoCov">          0 :     }</span></a>
<a name="264"><span class="lineNum">     264 </span><span class="lineNoCov">          0 : }</span></a>
</pre>
      </td>
    </tr>
  </table>
  <br>

  <table width="100%" border=0 cellspacing=0 cellpadding=0>
    <tr><td class="ruler"><img src="../../glass.png" width=3 height=3 alt=""></td></tr>
    <tr><td class="versionInfo">Generated by: <a href="http://ltp.sourceforge.net/coverage/lcov.php" target="_parent">LCOV version 1.14</a></td></tr>
  </table>
  <br>

</body>
</html>
